{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBb98xrojuO67JS0h7mexZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinavKumar07/C126/blob/main/C126.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eppAuX3wHxIn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "rewards = np.array([\n",
        "    [-1,-1,-1,-1,0,-1],\n",
        "    [-1,-1,-1,0,-1,100],\n",
        "    [-1,-1,-1,0,-1,-1],\n",
        "    [-1,0,0,-1,-1,-1],\n",
        "    [0,-1,-1,-1,-1,100],\n",
        "    [-1,-1,-1,-1,0,100]\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_initial_state():\n",
        "  return np.random.randint(0,6)\n",
        "\n",
        "set_initial_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I9_QO6uJL0G",
        "outputId": "df852117-27a5-48a4-ddba-c96fcdc57d10"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(current_state , reward_matrix):\n",
        "  available_action = []\n",
        "  print(\"Reward_matrix\",\"\\n\" , reward_matrix)\n",
        "\n",
        "  for action in enumerate(reward_matrix[current_state]):\n",
        "    if action[1] != -1 :\n",
        "      available_action.append(action[0])\n",
        "\n",
        "  choose_action = random.choice(available_action)\n",
        "  print(\"Random choice of action from\" , available_action , \"is\" , choose_action)\n",
        "  return choose_action"
      ],
      "metadata": {
        "id": "gn3TLIgwJpjl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#current_state = 4\n",
        "#action = get_action(current_state , rewards)\n",
        "#print(action)"
      ],
      "metadata": {
        "id": "8JMRDF_CK_V2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_state = 5\n",
        "action = get_action(current_state , rewards)\n",
        "print(action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNCQHeKwLU4V",
        "outputId": "2eac3049-86a4-47e9-9e19-cc642e4e3d5a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [4, 5] is 5\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#current_state = 0\n",
        "#action = get_action(current_state , rewards)\n",
        "#print(action)"
      ],
      "metadata": {
        "id": "qAUjx22bLr0j"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#current_state = 1\n",
        "#action = get_action(current_state , rewards)\n",
        "#print(action)"
      ],
      "metadata": {
        "id": "_LVK4wgjLtqf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C125\n"
      ],
      "metadata": {
        "id": "Qlv1sub3DFqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_matrix = np.zeros([6,6])\n",
        "print(q_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HO4Mbi0DGUh",
        "outputId": "c8d9e97a-47e0-4d3b-aafe-2cdf42bf4ab9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.8\n",
        "def take_action(current_state,reward_matrix,gamma):\n",
        "  action = get_action(current_state,reward_matrix)\n",
        "  current_reward = reward_matrix[current_state,action]\n",
        "  print(\"Current reward:\" , current_reward)\n",
        "\n",
        "  q_value = max(q_matrix[action])\n",
        "  print(\"q-value:\" , q_value)\n",
        "\n",
        "  q_current_state = current_reward + (gamma * q_value)\n",
        "  print(\"Q-current-state\",q_current_state)\n",
        "\n",
        "  q_matrix[current_state,action] = q_current_state\n",
        "  print(\"Q-matrix:\" ,\"\\n\" , q_matrix)\n",
        "\n",
        "  new_state = action\n",
        "\n",
        "  print(\"**********************************************************\")\n",
        "\n",
        "  if(new_state == 5):\n",
        "    print(\"Reached goal\")\n",
        "  else:\n",
        "    print(f\"Old state:{current_state} New state: {new_state}\")\n",
        "  return new_state\n",
        "\n",
        "take_action(current_state,rewards,gamma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15fdVYu4Ds_-",
        "outputId": "7c4a2cb2-417f-4403-b3f1-c22bc45f22a0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [4, 5] is 4\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "**********************************************************\n",
            "Old state:5 New state: 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#C126\n",
        "\n",
        "def run_episode(initial_state , reward_matrix, gamma):\n",
        "  new_state = take_action(initial_state , reward_matrix , gamma)\n",
        "  while True:\n",
        "    if(new_state == 5):\n",
        "      break\n",
        "    else:\n",
        "      new_state = take_action(new_state,reward_matrix,gamma)\n",
        "    print(new_state)\n"
      ],
      "metadata": {
        "id": "0tGtJmJOdeNi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(episodes , reward_matrix , gamma):\n",
        "  print(\"train:\")\n",
        "  for episode in range(episodes):\n",
        "\n",
        "    print(\"Starting episode:\"  , episode)\n",
        "    initial_state = set_initial_state()\n",
        "    print(\"Initial_state:\" , initial_state)\n",
        "\n",
        "    if(initial_state != 5 ):\n",
        "      run_episode(initial_state , reward_matrix ,gamma)\n",
        "    print(\"Ending episode:\" , episode)\n",
        "\n",
        "  print(\"Training completed\")\n",
        "  return q_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "4PFUd15cfycg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.8\n",
        "q_table = train(20,rewards, gamma)\n",
        "print(\"Final q-table:\\n\" , q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENRY_jN8g39c",
        "outputId": "c91147e3-9ccb-439a-bff9-496566bc1004"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:\n",
            "Starting episode: 0\n",
            "Initial_state: 0\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [4] is 4\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "**********************************************************\n",
            "Old state:0 New state: 4\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [0, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 0\n",
            "Starting episode: 1\n",
            "Initial_state: 5\n",
            "Ending episode: 1\n",
            "Starting episode: 2\n",
            "Initial_state: 2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 2\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 2\n",
            "2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 3\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:1 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 2\n",
            "Starting episode: 3\n",
            "Initial_state: 1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 3\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:1 New state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.  64.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:1 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.  64.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.  64.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:1 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.  64.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.  64.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 3\n",
            "Starting episode: 4\n",
            "Initial_state: 1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.  64.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "Ending episode: 4\n",
            "Starting episode: 5\n",
            "Initial_state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 2\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.  64.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.  64.   0. 100.]\n",
            " [  0.   0.   0.  64.   0.   0.]\n",
            " [  0.  80.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 100.]\n",
            " [  0.   0.   0.   0.   0.   0.]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 2\n",
            "Current reward: 0\n",
            "q-value: 64.0\n",
            "Q-current-state 51.2\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 2\n",
            "2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 5\n",
            "Starting episode: 6\n",
            "Initial_state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 6\n",
            "Starting episode: 7\n",
            "Initial_state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 7\n",
            "Starting episode: 8\n",
            "Initial_state: 5\n",
            "Ending episode: 8\n",
            "Starting episode: 9\n",
            "Initial_state: 4\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [0, 5] is 0\n",
            "Current reward: 0\n",
            "q-value: 0.0\n",
            "Q-current-state 0.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.    0.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:4 New state: 0\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [4] is 4\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [  0.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:0 New state: 4\n",
            "4\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [0, 5] is 0\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:4 New state: 0\n",
            "0\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [4] is 4\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:0 New state: 4\n",
            "4\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [0, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 9\n",
            "Starting episode: 10\n",
            "Initial_state: 2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 2\n",
            "Current reward: 0\n",
            "q-value: 64.0\n",
            "Q-current-state 51.2\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 2\n",
            "2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 2\n",
            "Current reward: 0\n",
            "q-value: 64.0\n",
            "Q-current-state 51.2\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 2\n",
            "2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:1 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 10\n",
            "Starting episode: 11\n",
            "Initial_state: 1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "Ending episode: 11\n",
            "Starting episode: 12\n",
            "Initial_state: 5\n",
            "Ending episode: 12\n",
            "Starting episode: 13\n",
            "Initial_state: 5\n",
            "Ending episode: 13\n",
            "Starting episode: 14\n",
            "Initial_state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 14\n",
            "Starting episode: 15\n",
            "Initial_state: 0\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [4] is 4\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:0 New state: 4\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [0, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 15\n",
            "Starting episode: 16\n",
            "Initial_state: 0\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [4] is 4\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:0 New state: 4\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [0, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 16\n",
            "Starting episode: 17\n",
            "Initial_state: 2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 2\n",
            "Current reward: 0\n",
            "q-value: 64.0\n",
            "Q-current-state 51.2\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 2\n",
            "2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 2\n",
            "Current reward: 0\n",
            "q-value: 64.0\n",
            "Q-current-state 51.2\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 2\n",
            "2\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:2 New state: 3\n",
            "3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 17\n",
            "Starting episode: 18\n",
            "Initial_state: 1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 3\n",
            "Current reward: 0\n",
            "q-value: 80.0\n",
            "Q-current-state 64.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:1 New state: 3\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [1, 2] is 1\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:3 New state: 1\n",
            "1\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [3, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 18\n",
            "Starting episode: 19\n",
            "Initial_state: 0\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [4] is 4\n",
            "Current reward: 0\n",
            "q-value: 100.0\n",
            "Q-current-state 80.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Old state:0 New state: 4\n",
            "Reward_matrix \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Random choice of action from [0, 5] is 5\n",
            "Current reward: 100\n",
            "q-value: 0.0\n",
            "Q-current-state 100.0\n",
            "Q-matrix: \n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n",
            "**********************************************************\n",
            "Reached goal\n",
            "5\n",
            "Ending episode: 19\n",
            "Training completed\n",
            "Final q-table:\n",
            " [[  0.    0.    0.    0.   80.    0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.   51.2   0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  100. ]\n",
            " [  0.    0.    0.    0.    0.    0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_path(q_table):\n",
        "  path = []\n",
        "  reward = []\n",
        "  total_reward = 0\n",
        "  initial_state = set_initial_state()\n",
        "  print(initial_state)\n",
        "\n",
        "  max_value_index = np.argmax(q_table[initial_state])\n",
        "  max_value = np.max(q_table[initial_state])\n",
        "  path.append(max_value_index)\n",
        "  reward.append(max_value)\n",
        "  while max_value_index != 5:\n",
        "    max_value_index = np.argmax(q_table[max_value_index])\n",
        "    max_value = np.argmax(q_table[max_value_index])\n",
        "\n",
        "    path.append(max_value_index)\n",
        "    reward.append(max_value)\n",
        "\n",
        "  total_reward = sum(reward)\n",
        "\n",
        "  return path, total_reward\n",
        "\n",
        "q_optimal_path , max_reward = optimal_path(q_table)\n",
        "\n",
        "print(q_optimal_path , max_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "306QBGIihlwj",
        "outputId": "bd196cd8-33a4-434f-9e4c-ae2202d8b407"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "[5] 100.0\n"
          ]
        }
      ]
    }
  ]
}